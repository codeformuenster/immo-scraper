#!/usr/bin/env python
""" Aggregate previously scraped data. """
import itertools
import json
import logging
import sys
from pathlib import Path

import pandas as pd

from immo_scraper.io.paths import (
    AGGREGATED_CSV,
    ALERTS_IDS,
    AGGREGATED_CSV,
    DIR_RAW,
    DIR_PROCESSED,
)
from immo_scraper.io.bucket import download_raw_data_from_bucket

logging.basicConfig(stream=sys.stdout, level=logging.INFO)

# download raw data from bucket
download_raw_data_from_bucket()

# read raw data
filenames = [str(key) for key in DIR_RAW.glob("*") if key.name != ".gitkeep"]

filedata = []
for filename in filenames:
    logging.debug(f"Reading file {filename}...")
    with open(filename, "r") as f:
        filedata.append(json.loads(f.read()))

# format data
entries_dicts = list(itertools.chain(*filedata))
df = (
    pd.DataFrame(entries_dicts)  # dicts to DataFrame
    .drop(columns=["updated_in_days", "updated_in_days_formatted"])  # drop columns
    .drop_duplicates()  # filter out duplicates
)

# ENGINEER FEATURES
# id of entry
df["id"] = df.apply(lambda row: row.lister_url.split("/")[4], axis=1)
# geo-coordinates
df["lat_long"] = df.apply(
    lambda row: f"{row.latitude: .3f} {row.longitude: .3f}", axis=1
)
# time of scraping
# TODO

# write result to bucket
DIR_PROCESSED.mkdir(
    parents=True, exist_ok=True
)  # create target directory, if not exists
df.to_csv(str(AGGREGATED_CSV), index=False)
